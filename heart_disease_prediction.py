# -*- coding: utf-8 -*-
"""Heart_Disease_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V76NBd-0rnEvPGUES9GtoNgLEivAigyi
"""

#loading the required Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

#Loading the dataset
data = pd.read_csv('heart_disease.csv')
data.head()

data.shape

#Loading the info about the dataset
data.info()

#Obtaining the statistics
data.describe()

# Checking whether the data is having any null values
data.isnull().sum()

# Checking the data distribution of oldpeak feature
sns.histplot(data['oldpeak'],bins = 20,kde = True)
plt.tight_layout()
plt.show()

# Checking whether the oldpeak is having any outliers
sns.boxplot(data['oldpeak'])
plt.tight_layout()
plt.show()

# Applying median imputation method on the oldpeak as it has ouliers and is right skewed.
data['oldpeak'] = data['oldpeak'].fillna(data['oldpeak'].median())

data.isnull().sum()

feature_cols = data.select_dtypes(include = ['int64','float64']).columns
feature_cols
for col in feature_cols:
  sns.boxplot(data[col])
  plt.tight_layout()
  plt.show()

for col in feature_cols:
  sns.histplot(data[col],bins = 20,kde = True)
  plt.tight_layout()
  plt.show()

# Encoding the categorical variables using one-hot encoding
categorical_cols = data.select_dtypes(include = ['object','bool']).columns
categorical_cols

data = pd.get_dummies(data,drop_first = True)
data.head()

# Splitting the data using train_test_split
from sklearn.model_selection import train_test_split

features = data.drop('num',axis = 1)
x = features
y = data['num']
y_binary = y.apply(lambda x: 1 if x > 0 else 0)
x_train,x_test,y_train,y_test = train_test_split(x,y_binary,test_size = 0.2,random_state = 42)

# Building the model
# Importing decision tree classifier as target variable is categorical,if target variable is continuous and numerical then regressor has be used
from sklearn.tree import DecisionTreeClassifier
dt_model = DecisionTreeClassifier(random_state = 42)
dt_model.fit(x_train,y_train)
# obtaining y_pred from the x_test
y_pred = dt_model.predict(x_test)

# Evaluation metrics
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
# Accuracy
print("Accuracy:", round(accuracy_score(y_test, y_pred),2))

# Precision
print("Precision:", round(precision_score(y_test, y_pred,average ='macro'),2))

# Recall
print("Recall:", round(recall_score(y_test, y_pred,average = 'macro'),2))

# F1-Score
print("F1-Score:", round(f1_score(y_test, y_pred,average = 'macro'),2))

# roc_auc_score
# This is a multiclass classification problem so we need probabilities of predicted values rather than just giving predicted values
y_prob = dt_model.predict_proba(x_test)
# Converting all the labels to binary format of y_test which are actual answers
from sklearn.preprocessing import label_binarize
y_test_roc = label_binarize(y_test, classes=[0,1,2,3,4])
print('roc_auc_score:',round(roc_auc_score(y_test_roc, y_prob,multi_class = 'ovo',average = 'macro'),2))

# Hyperparameter Tuning
# Using gridsearchcv to find the best possible combinations of hyperparameters
from sklearn.model_selection import GridSearchCV
param_grid = {
    'criterion': ['gini', 'entropy'],
    'max_depth': [3, 5, 7, 10, None], # None means the tree can grow as deep as it wants
    'min_samples_split': [2, 5, 10]


}
dt = DecisionTreeClassifier(random_state =42)
# Building GridSearchCV on Decisiontreeclassifier
grid = GridSearchCV(estimator=dt,param_grid=param_grid,cv=5)
grid.fit(x_train,y_train)

# Obtaining the best  parameters
best_params = grid.best_params_
print( best_params)
print("Best Score:", round(grid.best_score_,2))

"""Model Evaluation and Analysis:Accuracy: 0.46
Precision: 0.26
Recall: 0.27
F1-Score: 0.26
roc_auc_score: 0.55
The scores of the Decision tree model is low when considering all the metrics that means the model performance is poor.
From Accuracy to roc_auc_score the scores are poor and the model prediction is equal to a random guess. The model is not properly separating the positive and negative classes which means models predictive predictive power is poor. A model like this with scores like this  is not suitable for real world applications  as it would not be reliable.Hence model performance very poor
Accuracy after hyperparameter tuning:
Hyperparamter tuning is the process of finding the best parameters for a model so that model performance is improved.I have used gridsearchcv to find the best parameters and i have got accuracy of 56%.Which is 10% higher when compared with previous accuracy before the hyperparameter tuning.Hence it's no doubt that hyperparameter tuning increases the model performance.Overall tuning helps the model to perform well with its best performance but 56% accuracy is still not the best performance a model could achieve.
"""

# Visualizing the decision tree
# Observing decision tree visually with tuned parameters
model_to_visualize = DecisionTreeClassifier(criterion = 'gini',max_depth = 3,min_samples_split = 2)
model_to_visualize.fit(x_train,y_train)
# plotting the decision tree
from sklearn.tree import plot_tree
plt.figure(figsize =(25,15))
plot_tree(model_to_visualize,
         feature_names = features.columns,
          class_names = ['No disease','has disease'],
          filled = True,
          rounded = True,
          fontsize = 10)
plt.title('Decision Tree for Heart disease prediction',fontsize =20)
plt.show()

"""Important features : Important features are thal_reversable defect,cp_non-anginal,oldpeak,age,chol.

Hyperparameters of Decision tree


1.  max_depth:Low value → shallow tree → underfitting → high bias, low variance.High value → deep tree → overfitting → low bias, high variance.
2.  min_samples_split:Low value (e.g 2) → tree can split often → may overfit.High value → splits only with many samples → simpler tree → may underfit.


3.criterion:Slight differences in splits; doesn't usually have a big effect unless tree is very small or dataset is imbalanced.

Difference between label encoding and one-hot encoding:
 difference is that Label Encoding converts categories into a single column with numerical values (e.g., 1, 2, 3), Good for ordinal variables where order matters. For nominal data, it may introduce false order. One-Hot Encoding creates new binary (0 or 1) columns for each category, avoiding this problem.
"""